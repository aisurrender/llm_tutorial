{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 3: Pretrain - 预训练语言模型\n",
    "\n",
    "## 学习目标\n",
    "\n",
    "1. 理解预训练的目标：**下一个词预测**\n",
    "2. **动手实现**数据集的构建\n",
    "3. **动手实现**学习率调度（Warmup + Cosine Decay）\n",
    "4. **动手实现**训练循环\n",
    "\n",
    "## 学习方式\n",
    "\n",
    "1. 在这个 Notebook 中学习概念\n",
    "2. 去 `data_exercise.py` 完成数据集 TODO\n",
    "3. 去 `train_exercise.py` 完成训练 TODO\n",
    "4. 卡住了？查看 `*_solution.py`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 1. 语言建模：下一个词预测\n",
    "\n",
    "GPT 的预训练目标很简单：**给定前文，预测下一个词**\n",
    "\n",
    "```\n",
    "输入: \"今天天气\"\n",
    "目标: \"天气真好\"\n",
    "\n",
    "位置 0: 输入\"今\" -> 预测\"天\"\n",
    "位置 1: 输入\"今天\" -> 预测\"气\"\n",
    "位置 2: 输入\"今天天\" -> 预测\"真\"\n",
    "位置 3: 输入\"今天天气\" -> 预测\"好\"\n",
    "```\n",
    "\n",
    "损失函数是**交叉熵**：\n",
    "```python\n",
    "loss = -sum(log P(下一个词 | 前文))\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import math\n",
    "\n",
    "# 演示：语言建模目标\n",
    "# 假设词表大小为 10，序列长度为 4\n",
    "\n",
    "# 模型输出的 logits（每个位置预测下一个词的分数）\n",
    "logits = torch.randn(1, 4, 10)  # [batch, seq_len, vocab_size]\n",
    "\n",
    "# 目标序列（真实的下一个词）\n",
    "targets = torch.tensor([[2, 5, 3, 8]])  # [batch, seq_len]\n",
    "\n",
    "# 计算交叉熵损失\n",
    "loss = F.cross_entropy(\n",
    "    logits.view(-1, 10),  # [batch * seq_len, vocab_size]\n",
    "    targets.view(-1)      # [batch * seq_len]\n",
    ")\n",
    "\n",
    "print(f\"Logits shape: {logits.shape}\")\n",
    "print(f\"Targets shape: {targets.shape}\")\n",
    "print(f\"Loss: {loss.item():.4f}\")\n",
    "print(f\"\\n理论最小 loss（完美预测）: 0\")\n",
    "print(f\"随机预测的 loss: {math.log(10):.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 2. 数据集构建\n",
    "\n",
    "### 2.1 输入和目标的关系\n",
    "\n",
    "```\n",
    "文本:  h e l l o   w o r l d\n",
    "位置:  0 1 2 3 4 5 6 7 8 9 10\n",
    "\n",
    "样本 0 (block_size=4):\n",
    "  输入: [h, e, l, l]  (位置 0-3)\n",
    "  目标: [e, l, l, o]  (位置 1-4)\n",
    "  \n",
    "样本 1:\n",
    "  输入: [e, l, l, o]  (位置 1-4)\n",
    "  目标: [l, l, o,  ]  (位置 2-5)\n",
    "```\n",
    "\n",
    "**关键**：目标是输入右移一位！"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 演示数据集构建\n",
    "text = \"hello world\"\n",
    "block_size = 4\n",
    "\n",
    "# 简单的字符编码\n",
    "chars = sorted(set(text))\n",
    "char_to_idx = {c: i for i, c in enumerate(chars)}\n",
    "data = torch.tensor([char_to_idx[c] for c in text])\n",
    "\n",
    "print(f\"文本: {text}\")\n",
    "print(f\"编码: {data.tolist()}\")\n",
    "print(f\"词表: {char_to_idx}\")\n",
    "print()\n",
    "\n",
    "# 构建样本\n",
    "for idx in range(3):\n",
    "    x = data[idx : idx + block_size]\n",
    "    y = data[idx + 1 : idx + block_size + 1]\n",
    "    \n",
    "    x_text = ''.join([chars[i] for i in x.tolist()])\n",
    "    y_text = ''.join([chars[i] for i in y.tolist()])\n",
    "    \n",
    "    print(f\"样本 {idx}: 输入='{x_text}' -> 目标='{y_text}'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 练习：实现 __getitem__\n",
    "\n",
    "去 `data_exercise.py`，完成 **TODO 1**：实现数据集的 `__getitem__` 方法"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 测试你的实现\n",
    "import importlib\n",
    "import data_exercise\n",
    "importlib.reload(data_exercise)\n",
    "\n",
    "# 创建示例数据（如果不存在）\n",
    "import os\n",
    "if not os.path.exists(\"sample_data.txt\"):\n",
    "    data_exercise.create_sample_data(\"sample_data.txt\")\n",
    "\n",
    "# 测试数据集\n",
    "data_exercise.test_dataset()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 3. 学习率调度\n",
    "\n",
    "### 3.1 为什么需要学习率调度？\n",
    "\n",
    "**Warmup（预热）**：\n",
    "- 训练初期，模型参数是随机的\n",
    "- 梯度可能很大且不稳定\n",
    "- 从小学习率开始，逐渐增大\n",
    "\n",
    "**Cosine Decay（余弦衰减）**：\n",
    "- 训练后期需要更小的学习率\n",
    "- 进行精细调整，收敛到更好的最优点\n",
    "\n",
    "```\n",
    "lr\n",
    "↑\n",
    "|   /\\\n",
    "|  /  \\\n",
    "| /    \\___________\n",
    "|/\n",
    "+------------------→ step\n",
    "  warmup   decay\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# 演示学习率调度\n",
    "def demo_lr_schedule(warmup_steps, total_steps, max_lr, min_lr):\n",
    "    steps = list(range(total_steps))\n",
    "    lrs = []\n",
    "    \n",
    "    for step in steps:\n",
    "        if step < warmup_steps:\n",
    "            # Warmup: 线性增加\n",
    "            lr = max_lr * (step + 1) / warmup_steps\n",
    "        else:\n",
    "            # Cosine decay\n",
    "            progress = (step - warmup_steps) / (total_steps - warmup_steps)\n",
    "            lr = min_lr + 0.5 * (max_lr - min_lr) * (1 + math.cos(math.pi * progress))\n",
    "        lrs.append(lr)\n",
    "    \n",
    "    return steps, lrs\n",
    "\n",
    "# 绘制\n",
    "steps, lrs = demo_lr_schedule(100, 1000, 1e-3, 1e-4)\n",
    "\n",
    "plt.figure(figsize=(10, 4))\n",
    "plt.plot(steps, lrs)\n",
    "plt.axvline(x=100, color='r', linestyle='--', label='Warmup 结束')\n",
    "plt.xlabel('Step')\n",
    "plt.ylabel('Learning Rate')\n",
    "plt.title('Warmup + Cosine Decay')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 练习：实现学习率调度\n",
    "\n",
    "去 `train_exercise.py`，完成 **TODO 2a, 2b**：\n",
    "\n",
    "- 2a: Warmup 阶段的线性增加\n",
    "- 2b: Cosine Decay 阶段的衰减"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 测试学习率调度\n",
    "import train_exercise\n",
    "importlib.reload(train_exercise)\n",
    "\n",
    "train_exercise.test_lr_schedule()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 4. 训练循环\n",
    "\n",
    "### 4.1 核心步骤\n",
    "\n",
    "```python\n",
    "for batch in dataloader:\n",
    "    # 1. 前向传播\n",
    "    logits, loss = model(input_ids, targets=labels)\n",
    "    \n",
    "    # 2. 反向传播\n",
    "    loss.backward()\n",
    "    \n",
    "    # 3. 梯度裁剪（防止梯度爆炸）\n",
    "    torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "    \n",
    "    # 4. 更新参数\n",
    "    optimizer.step()\n",
    "    optimizer.zero_grad()\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2 练习：实现训练循环\n",
    "\n",
    "去 `train_exercise.py`，完成 **TODO 3a, 3b, 3c**：\n",
    "\n",
    "- 3a: 更新学习率\n",
    "- 3b: 前向传播\n",
    "- 3c: 反向传播 + 梯度裁剪 + 优化器更新"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 运行训练（完成 TODO 后）\n",
    "# 注意：需要先完成 step2 的 model_solution.py\n",
    "# !python train_exercise.py --device cpu --epochs 2 --batch_size 8"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 5. 梯度裁剪\n",
    "\n",
    "### 为什么需要梯度裁剪？\n",
    "\n",
    "- 深层网络中，梯度可能会\"爆炸\"（变得非常大）\n",
    "- 大梯度会导致参数更新过大，破坏已学到的知识\n",
    "- 梯度裁剪限制梯度的最大范数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 演示梯度裁剪\n",
    "import torch.nn as nn\n",
    "\n",
    "# 创建一个简单模型\n",
    "model = nn.Linear(10, 10)\n",
    "\n",
    "# 模拟一些梯度\n",
    "x = torch.randn(1, 10)\n",
    "y = model(x).sum()\n",
    "y.backward()\n",
    "\n",
    "# 手动放大梯度（模拟梯度爆炸）\n",
    "for p in model.parameters():\n",
    "    if p.grad is not None:\n",
    "        p.grad *= 100\n",
    "\n",
    "# 计算梯度范数\n",
    "grad_norm_before = torch.nn.utils.clip_grad_norm_(model.parameters(), float('inf'))\n",
    "print(f\"裁剪前梯度范数: {grad_norm_before:.2f}\")\n",
    "\n",
    "# 重新计算并裁剪\n",
    "model.zero_grad()\n",
    "y = model(x).sum()\n",
    "y.backward()\n",
    "for p in model.parameters():\n",
    "    if p.grad is not None:\n",
    "        p.grad *= 100\n",
    "\n",
    "grad_norm_after = torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "print(f\"裁剪后梯度范数: {grad_norm_after:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 6. 运行完整训练\n",
    "\n",
    "完成所有 TODO 后，运行完整的预训练："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 运行完整训练\n",
    "!python train_exercise.py --device cpu --epochs 3 --batch_size 8 --log_interval 20"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 7. 验证清单\n",
    "\n",
    "完成本步骤后，你应该能够：\n",
    "\n",
    "- [ ] 解释\"下一个词预测\"的训练目标\n",
    "- [ ] 实现数据集的 `__getitem__`\n",
    "- [ ] 解释为什么需要学习率调度（Warmup + Decay）\n",
    "- [ ] 实现学习率调度函数\n",
    "- [ ] 实现完整的训练循环\n",
    "\n",
    "---\n",
    "\n",
    "## 下一步\n",
    "\n",
    "预训练完成后，进入 [Step 4: SFT](../step4_sft/) 学习如何让模型学会遵循指令。"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
