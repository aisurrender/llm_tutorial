{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 1: Tokenizer - 文本如何变成数字\n",
    "\n",
    "## 学习目标\n",
    "\n",
    "1. 理解 Tokenizer 的作用\n",
    "2. **动手实现**字符级 Tokenizer 的 `encode` 和 `decode`\n",
    "3. **动手实现** BPE 算法的核心步骤\n",
    "4. 使用 SentencePiece 训练中文分词器\n",
    "\n",
    "## 学习方式\n",
    "\n",
    "1. 在这个 Notebook 中学习概念、运行代码、查看可视化\n",
    "2. 去 `tokenizer_exercise.py` 完成 TODO 部分的代码\n",
    "3. 回到这里验证你的实现\n",
    "4. 卡住了？查看 `tokenizer_solution.py` 中的答案"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 1. 为什么需要 Tokenizer？\n",
    "\n",
    "神经网络只能处理数字（张量），所以我们需要一种方式将文本转换为数字序列。\n",
    "\n",
    "```\n",
    "\"Hello world\" → Tokenizer → [15496, 995] → 神经网络 → [预测的token] → Tokenizer → \"Hi\"\n",
    "```\n",
    "\n",
    "关键要求：**这个映射必须是可逆的**（能从数字还原回文本）"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 2. 字符级 Tokenizer\n",
    "\n",
    "最简单的方案：每个字符对应一个数字\n",
    "\n",
    "### 2.1 理解词表 (Vocabulary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 导入你的实现（完成 TODO 后运行）\n",
    "from tokenizer_exercise import CharTokenizer\n",
    "\n",
    "# 创建分词器\n",
    "tokenizer = CharTokenizer()\n",
    "\n",
    "# 查看初始词表（只有特殊token）\n",
    "print(\"初始词表:\")\n",
    "print(tokenizer.vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 从文本构建词表\n",
    "text = \"Hello world! 你好世界！\"\n",
    "tokenizer.build_vocab(text)\n",
    "\n",
    "# 查看构建后的词表\n",
    "print(\"\\n构建后的词表:\")\n",
    "for char, idx in sorted(tokenizer.vocab.items(), key=lambda x: x[1]):\n",
    "    print(f\"  '{char}' -> {idx}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 练习：实现 encode 和 decode\n",
    "\n",
    "现在去 `tokenizer_exercise.py`，完成 **TODO 1** 和 **TODO 2**：\n",
    "\n",
    "- `encode(text)`: 将文本转换为 token ID 列表\n",
    "- `decode(ids)`: 将 token ID 列表还原为文本\n",
    "\n",
    "完成后，运行下面的代码验证："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 重新导入（如果你修改了代码）\n",
    "import importlib\n",
    "import tokenizer_exercise\n",
    "importlib.reload(tokenizer_exercise)\n",
    "from tokenizer_exercise import CharTokenizer\n",
    "\n",
    "# 测试你的实现\n",
    "tokenizer = CharTokenizer()\n",
    "tokenizer.build_vocab(\"Hello world! 你好世界！\")\n",
    "\n",
    "test_text = \"Hello 你好\"\n",
    "print(f\"原文: {test_text}\")\n",
    "\n",
    "try:\n",
    "    ids = tokenizer.encode(test_text)\n",
    "    print(f\"编码: {ids}\")\n",
    "    \n",
    "    decoded = tokenizer.decode(ids)\n",
    "    print(f\"解码: {decoded}\")\n",
    "    \n",
    "    if test_text == decoded:\n",
    "        print(\"\\n✅ 编码-解码一致，实现正确！\")\n",
    "    else:\n",
    "        print(f\"\\n❌ 编码-解码不一致: '{test_text}' != '{decoded}'\")\n",
    "except NotImplementedError as e:\n",
    "    print(f\"\\n⚠️ {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3 可视化：字符到数字的映射"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 可视化编码过程\n",
    "def visualize_encoding(tokenizer, text):\n",
    "    \"\"\"可视化文本编码过程\"\"\"\n",
    "    try:\n",
    "        ids = tokenizer.encode(text)\n",
    "        print(f\"文本: {text}\")\n",
    "        print(\"      \" + \"   \".join([f\"{c:^3}\" for c in text]))\n",
    "        print(\"       ↓   \" * len(text))\n",
    "        print(\"IDs:  \" + \"   \".join([f\"{i:^3}\" for i in ids]))\n",
    "    except NotImplementedError:\n",
    "        print(\"请先完成 encode 方法\")\n",
    "\n",
    "visualize_encoding(tokenizer, \"Hello\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 3. BPE (Byte Pair Encoding) 算法\n",
    "\n",
    "### 3.1 为什么需要 BPE？\n",
    "\n",
    "字符级分词的问题：\n",
    "- 序列太长（\"hello\" → 5 个 token）\n",
    "- 没有利用词的语义\n",
    "\n",
    "词级分词的问题：\n",
    "- 词表太大\n",
    "- OOV（未登录词）问题\n",
    "\n",
    "BPE 的解决方案：**子词级分词** —— 自动学习常见的字符组合\n",
    "\n",
    "```\n",
    "\"unhappiness\" → [\"un\", \"happi\", \"ness\"]  # 3 个有意义的子词\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 BPE 算法原理\n",
    "\n",
    "核心思想：**反复合并最高频的相邻 token 对**\n",
    "\n",
    "```\n",
    "初始: l o w (出现5次), l o w e r (出现2次)\n",
    "\n",
    "Step 1: 统计 pair 频率\n",
    "  (l, o): 7次, (o, w): 7次, (w, e): 2次, (e, r): 2次\n",
    "\n",
    "Step 2: 合并最高频的 (l, o) -> lo\n",
    "  lo w (5次), lo w e r (2次)\n",
    "\n",
    "Step 3: 继续统计和合并...\n",
    "  (lo, w): 7次 -> low\n",
    "  ...\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3 练习：实现 BPE 统计函数\n",
    "\n",
    "去 `tokenizer_exercise.py`，完成 **TODO 3**: `_get_stats` 方法\n",
    "\n",
    "这个函数统计所有相邻 token 对的出现频率。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 重新导入\n",
    "importlib.reload(tokenizer_exercise)\n",
    "from tokenizer_exercise import BPETokenizer\n",
    "\n",
    "# 测试 _get_stats\n",
    "tokenizer = BPETokenizer()\n",
    "\n",
    "# 输入：词列表，每个词是空格分隔的字符序列\n",
    "words = [\n",
    "    (\"l o w\", 5),      # \"low\" 出现 5 次\n",
    "    (\"l o w e r\", 2),  # \"lower\" 出现 2 次\n",
    "]\n",
    "\n",
    "try:\n",
    "    stats = tokenizer._get_stats(words)\n",
    "    print(\"统计结果:\")\n",
    "    for pair, count in sorted(stats.items(), key=lambda x: -x[1]):\n",
    "        print(f\"  {pair}: {count}次\")\n",
    "    \n",
    "    # 验证\n",
    "    expected = {('l', 'o'): 7, ('o', 'w'): 7, ('w', 'e'): 2, ('e', 'r'): 2}\n",
    "    if stats == expected:\n",
    "        print(\"\\n✅ _get_stats 实现正确！\")\n",
    "    else:\n",
    "        print(f\"\\n❌ 结果不匹配\")\n",
    "        print(f\"期望: {expected}\")\n",
    "except NotImplementedError as e:\n",
    "    print(f\"⚠️ {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.4 练习：完成 BPE 训练循环\n",
    "\n",
    "去 `tokenizer_exercise.py`，完成 **TODO 4a, 4b, 4c**：\n",
    "\n",
    "- 4a: 获取 pair 频率\n",
    "- 4b: 找到频率最高的 pair\n",
    "- 4c: 合并这个 pair"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 重新导入\n",
    "importlib.reload(tokenizer_exercise)\n",
    "from tokenizer_exercise import BPETokenizer\n",
    "\n",
    "# 训练 BPE\n",
    "tokenizer = BPETokenizer()\n",
    "\n",
    "text = \"\"\"\n",
    "low lower lowest lowly\n",
    "new newer newest newly\n",
    "show showed showing shown\n",
    "the quick brown fox jumps over the lazy dog\n",
    "the the the quick quick brown brown\n",
    "\"\"\" * 5  # 重复以增加频率\n",
    "\n",
    "try:\n",
    "    tokenizer.train(text, vocab_size=80, verbose=True)\n",
    "    print(\"\\n✅ BPE 训练完成！\")\n",
    "except NotImplementedError as e:\n",
    "    print(f\"\\n⚠️ {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.5 可视化：BPE 合并规则"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 查看学到的合并规则\n",
    "if hasattr(tokenizer, 'merges') and tokenizer.merges:\n",
    "    print(\"学到的合并规则（前15个）:\")\n",
    "    for i, (pair, merged) in enumerate(list(tokenizer.merges.items())[:15]):\n",
    "        print(f\"  {i+1}. '{pair[0]}' + '{pair[1]}' -> '{merged}'\")\n",
    "else:\n",
    "    print(\"还没有学到合并规则，请先完成训练\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 测试编码解码\n",
    "if hasattr(tokenizer, 'merges') and tokenizer.merges:\n",
    "    test_words = [\"lower\", \"showing\", \"newest\"]\n",
    "    \n",
    "    for word in test_words:\n",
    "        ids = tokenizer.encode(word)\n",
    "        tokens = [tokenizer.id_to_token[i] for i in ids]\n",
    "        decoded = tokenizer.decode(ids)\n",
    "        \n",
    "        print(f\"'{word}' -> {tokens} -> IDs: {ids} -> '{decoded}'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 4. 运行测试\n",
    "\n",
    "完成所有 TODO 后，运行完整测试："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 运行测试文件\n",
    "!python tokenizer_exercise.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 5. 对比：使用真实的 Tokenizer\n",
    "\n",
    "让我们看看 GPT-2 的 tokenizer 是如何工作的："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 安装 tiktoken（如果需要）\n",
    "# !pip install tiktoken\n",
    "\n",
    "try:\n",
    "    import tiktoken\n",
    "    \n",
    "    # GPT-2 使用的 tokenizer\n",
    "    enc = tiktoken.get_encoding(\"gpt2\")\n",
    "    \n",
    "    text = \"Hello world! 你好世界！\"\n",
    "    ids = enc.encode(text)\n",
    "    tokens = [enc.decode([i]) for i in ids]\n",
    "    \n",
    "    print(f\"文本: {text}\")\n",
    "    print(f\"Token 数量: {len(ids)}\")\n",
    "    print(f\"Tokens: {tokens}\")\n",
    "    print(f\"IDs: {ids}\")\n",
    "    print(f\"\\nGPT-2 词表大小: {enc.n_vocab}\")\n",
    "except ImportError:\n",
    "    print(\"请安装 tiktoken: pip install tiktoken\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 6. 验证清单\n",
    "\n",
    "完成本步骤后，你应该能够：\n",
    "\n",
    "- [ ] 解释 Tokenizer 的作用（文本 ↔ 数字）\n",
    "- [ ] 实现 `encode` 方法（文本 → ID 列表）\n",
    "- [ ] 实现 `decode` 方法（ID 列表 → 文本）\n",
    "- [ ] 解释 BPE 算法（统计频率 → 合并高频 pair）\n",
    "- [ ] 实现 BPE 的统计和训练循环\n",
    "\n",
    "---\n",
    "\n",
    "## 下一步\n",
    "\n",
    "理解 Tokenizer 后，进入 [Step 2: GPT Model](../step2_gpt_model/) 学习如何构建处理这些 token 的模型。"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
