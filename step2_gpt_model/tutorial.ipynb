{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 2: GPT Model - Transformer 架构\n",
    "\n",
    "## 学习目标\n",
    "\n",
    "1. 理解 Embedding（词嵌入 + 位置嵌入）\n",
    "2. **动手实现** Multi-Head Self-Attention\n",
    "3. **动手实现** Feed-Forward Network (MLP)\n",
    "4. **动手实现** 完整的 GPT 模型\n",
    "\n",
    "## 学习方式\n",
    "\n",
    "1. 在这个 Notebook 中学习概念、运行代码、查看可视化\n",
    "2. 去 `model_exercise.py` 完成 TODO 部分的代码\n",
    "3. 回到这里验证你的实现\n",
    "4. 卡住了？查看 `model_solution.py` 中的答案"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 1. GPT 架构概览\n",
    "\n",
    "```\n",
    "输入 Token IDs: [1, 45, 23, 89, 12]\n",
    "        ↓\n",
    "┌─────────────────────────────────────┐\n",
    "│  Token Embedding + Position Embedding │\n",
    "└─────────────────────────────────────┘\n",
    "        ↓\n",
    "┌─────────────────────────────────────┐\n",
    "│         Transformer Block × N         │\n",
    "│  ┌─────────────────────────────────┐  │\n",
    "│  │  LayerNorm → Attention → +残差   │  │\n",
    "│  │  LayerNorm → MLP → +残差         │  │\n",
    "│  └─────────────────────────────────┘  │\n",
    "└─────────────────────────────────────┘\n",
    "        ↓\n",
    "┌─────────────────────────────────────┐\n",
    "│    Final LayerNorm → LM Head          │\n",
    "└─────────────────────────────────────┘\n",
    "        ↓\n",
    "输出 Logits: [vocab_size] 的概率分布\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import math\n",
    "\n",
    "# 设置随机种子\n",
    "torch.manual_seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 2. Embedding 层\n",
    "\n",
    "将离散的 token ID 转换为连续的向量表示。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 参数设置\n",
    "vocab_size = 1000   # 词表大小\n",
    "n_embd = 64         # 嵌入维度\n",
    "block_size = 32     # 最大序列长度\n",
    "\n",
    "# Token Embedding: 每个 token ID 映射为一个向量\n",
    "tok_emb = nn.Embedding(vocab_size, n_embd)\n",
    "\n",
    "# Position Embedding: 每个位置映射为一个向量\n",
    "pos_emb = nn.Embedding(block_size, n_embd)\n",
    "\n",
    "# 模拟输入\n",
    "tokens = torch.tensor([[10, 20, 30, 40]])  # [1, 4]\n",
    "positions = torch.arange(4)  # [0, 1, 2, 3]\n",
    "\n",
    "# 查看 embedding 结果\n",
    "print(f\"Token IDs: {tokens}\")\n",
    "print(f\"Token Embedding shape: {tok_emb(tokens).shape}\")\n",
    "print(f\"Position Embedding shape: {pos_emb(positions).shape}\")\n",
    "\n",
    "# 最终输入 = Token Embedding + Position Embedding\n",
    "x = tok_emb(tokens) + pos_emb(positions)\n",
    "print(f\"\\n最终输入 shape: {x.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 3. Self-Attention 机制\n",
    "\n",
    "Self-Attention 的核心：让每个 token \"看到\"序列中的其他 token\n",
    "\n",
    "### 3.1 Q, K, V 的含义\n",
    "\n",
    "- **Query (Q)**: \"我在找什么？\"\n",
    "- **Key (K)**: \"我有什么？\" \n",
    "- **Value (V)**: \"我的内容是什么？\"\n",
    "\n",
    "计算过程：\n",
    "```\n",
    "Attention = softmax(Q @ K^T / sqrt(d_k)) @ V\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 手动实现简单的 Self-Attention（单头）\n",
    "B, T, C = 1, 4, 64  # batch=1, seq_len=4, dim=64\n",
    "x = torch.randn(B, T, C)\n",
    "\n",
    "# Q, K, V 投影\n",
    "W_q = nn.Linear(C, C, bias=False)\n",
    "W_k = nn.Linear(C, C, bias=False)\n",
    "W_v = nn.Linear(C, C, bias=False)\n",
    "\n",
    "Q = W_q(x)  # [1, 4, 64]\n",
    "K = W_k(x)  # [1, 4, 64]\n",
    "V = W_v(x)  # [1, 4, 64]\n",
    "\n",
    "print(f\"Q shape: {Q.shape}\")\n",
    "print(f\"K shape: {K.shape}\")\n",
    "print(f\"V shape: {V.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 计算注意力分数\n",
    "# att = Q @ K^T / sqrt(d_k)\n",
    "att = Q @ K.transpose(-2, -1) / math.sqrt(C)\n",
    "print(f\"注意力分数 shape: {att.shape}\")  # [1, 4, 4]\n",
    "print(f\"\\n注意力分数矩阵:\")\n",
    "print(att[0].detach().numpy().round(2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 因果掩码（Causal Mask）\n",
    "\n",
    "GPT 是自回归模型：**每个 token 只能看到它之前的 token**\n",
    "\n",
    "通过上三角掩码实现：\n",
    "```\n",
    "     pos0  pos1  pos2  pos3\n",
    "pos0  ✓     ✗     ✗     ✗    (只能看自己)\n",
    "pos1  ✓     ✓     ✗     ✗    (看 pos0, pos1)\n",
    "pos2  ✓     ✓     ✓     ✗    (看 pos0, pos1, pos2)\n",
    "pos3  ✓     ✓     ✓     ✓    (看所有)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 创建因果掩码\n",
    "mask = torch.triu(torch.ones(T, T), diagonal=1).bool()\n",
    "print(\"因果掩码（True 表示要屏蔽的位置）:\")\n",
    "print(mask.int().numpy())\n",
    "\n",
    "# 应用掩码：将上三角设为 -inf\n",
    "att_masked = att.masked_fill(mask, float('-inf'))\n",
    "print(f\"\\n应用掩码后:\")\n",
    "print(att_masked[0].detach().numpy().round(2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Softmax 后，-inf 变成 0\n",
    "att_weights = F.softmax(att_masked, dim=-1)\n",
    "print(\"Softmax 后的注意力权重:\")\n",
    "print(att_weights[0].detach().numpy().round(2))\n",
    "\n",
    "# 最终输出\n",
    "output = att_weights @ V\n",
    "print(f\"\\n输出 shape: {output.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3 练习：实现 CausalSelfAttention\n",
    "\n",
    "现在去 `model_exercise.py`，完成 **TODO 3a, 3b, 3c**：\n",
    "\n",
    "- 3a: 计算注意力分数 `att = Q @ K^T / sqrt(d_k)`\n",
    "- 3b: 应用因果掩码\n",
    "- 3c: Softmax + Dropout + 与 V 相乘"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 重新导入测试\n",
    "import importlib\n",
    "import model_exercise\n",
    "importlib.reload(model_exercise)\n",
    "from model_exercise import CausalSelfAttention, GPTConfig\n",
    "\n",
    "config = GPTConfig(n_embd=128, n_head=4)\n",
    "attn = CausalSelfAttention(config)\n",
    "\n",
    "x = torch.randn(2, 32, 128)\n",
    "try:\n",
    "    out = attn(x)\n",
    "    print(f\"输入 shape: {x.shape}\")\n",
    "    print(f\"输出 shape: {out.shape}\")\n",
    "    print(\"\\n✅ CausalSelfAttention 实现正确!\")\n",
    "except NotImplementedError as e:\n",
    "    print(f\"⚠️ {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 4. MLP（前馈网络）\n",
    "\n",
    "结构：Linear → GELU → Linear\n",
    "\n",
    "先扩展到 4 倍维度，再压缩回来。这是 Transformer 中的\"思考\"部分。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MLP 结构演示\n",
    "n_embd = 128\n",
    "\n",
    "c_fc = nn.Linear(n_embd, 4 * n_embd)    # 扩展到 512\n",
    "gelu = nn.GELU()\n",
    "c_proj = nn.Linear(4 * n_embd, n_embd)  # 压缩回 128\n",
    "\n",
    "x = torch.randn(2, 32, 128)\n",
    "print(f\"输入: {x.shape}\")\n",
    "print(f\"扩展: {c_fc(x).shape}\")\n",
    "print(f\"GELU: {gelu(c_fc(x)).shape}\")\n",
    "print(f\"输出: {c_proj(gelu(c_fc(x))).shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1 练习：实现 MLP\n",
    "\n",
    "去 `model_exercise.py`，完成 **TODO 1**：实现 MLP 的 forward 方法"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 测试 MLP\n",
    "importlib.reload(model_exercise)\n",
    "from model_exercise import MLP, GPTConfig\n",
    "\n",
    "config = GPTConfig(n_embd=128)\n",
    "mlp = MLP(config)\n",
    "\n",
    "x = torch.randn(2, 32, 128)\n",
    "try:\n",
    "    out = mlp(x)\n",
    "    print(f\"输入 shape: {x.shape}\")\n",
    "    print(f\"输出 shape: {out.shape}\")\n",
    "    print(\"\\n✅ MLP 实现正确!\")\n",
    "except NotImplementedError as e:\n",
    "    print(f\"⚠️ {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 5. Transformer Block\n",
    "\n",
    "结构（Pre-LayerNorm + Residual）:\n",
    "```python\n",
    "x = x + Attention(LayerNorm(x))  # 残差连接\n",
    "x = x + MLP(LayerNorm(x))        # 残差连接\n",
    "```\n",
    "\n",
    "残差连接让梯度可以直接流过，使深层网络更容易训练。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.1 练习：实现 TransformerBlock\n",
    "\n",
    "去 `model_exercise.py`，完成 **TODO 2**：实现 TransformerBlock 的 forward 方法"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 测试 TransformerBlock\n",
    "importlib.reload(model_exercise)\n",
    "from model_exercise import TransformerBlock, GPTConfig\n",
    "\n",
    "config = GPTConfig(n_embd=128, n_head=4)\n",
    "block = TransformerBlock(config)\n",
    "\n",
    "x = torch.randn(2, 32, 128)\n",
    "try:\n",
    "    out = block(x)\n",
    "    print(f\"输入 shape: {x.shape}\")\n",
    "    print(f\"输出 shape: {out.shape}\")\n",
    "    print(\"\\n✅ TransformerBlock 实现正确!\")\n",
    "except NotImplementedError as e:\n",
    "    print(f\"⚠️ {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 6. 完整的 GPT 模型\n",
    "\n",
    "### 6.1 练习：实现 GPT forward\n",
    "\n",
    "去 `model_exercise.py`，完成 **TODO 4a, 4b, 4c**：\n",
    "\n",
    "- 4a: Token Embedding + Position Embedding\n",
    "- 4b: 通过所有 Transformer Blocks\n",
    "- 4c: Final LayerNorm + LM Head"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 测试完整 GPT\n",
    "importlib.reload(model_exercise)\n",
    "from model_exercise import GPT, GPTConfig\n",
    "\n",
    "config = GPTConfig(\n",
    "    vocab_size=1000,\n",
    "    block_size=64,\n",
    "    n_embd=128,\n",
    "    n_head=4,\n",
    "    n_layer=4\n",
    ")\n",
    "\n",
    "try:\n",
    "    model = GPT(config)\n",
    "    print(f\"参数量: {model.get_num_params()/1e6:.2f}M\")\n",
    "    \n",
    "    # 测试前向传播\n",
    "    idx = torch.randint(0, 1000, (2, 32))\n",
    "    logits, loss = model(idx, idx)\n",
    "    \n",
    "    print(f\"\\n输入 shape: {idx.shape}\")\n",
    "    print(f\"输出 logits shape: {logits.shape}\")\n",
    "    print(f\"Loss: {loss.item():.4f}\")\n",
    "    print(\"\\n✅ GPT 实现正确!\")\n",
    "except NotImplementedError as e:\n",
    "    print(f\"⚠️ {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.2 测试生成"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 测试生成（如果 GPT 实现正确）\n",
    "try:\n",
    "    start = torch.tensor([[1, 2, 3]])  # 起始 tokens\n",
    "    generated = model.generate(start, max_new_tokens=10, temperature=1.0)\n",
    "    print(f\"起始: {start.tolist()}\")\n",
    "    print(f\"生成: {generated.tolist()}\")\n",
    "except:\n",
    "    print(\"请先完成 GPT 的实现\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 7. 参数量分析\n",
    "\n",
    "GPT 参数量主要来自：\n",
    "- Embedding: vocab_size × n_embd\n",
    "- Attention: 4 × n_embd² per layer (Q, K, V, Output)\n",
    "- MLP: 8 × n_embd² per layer (扩展 + 压缩)\n",
    "\n",
    "**估算公式**: 约 12 × n_layer × n_embd² 个参数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 不同规模的 GPT\n",
    "configs = [\n",
    "    (\"Small\", GPTConfig(vocab_size=6400, n_embd=512, n_head=8, n_layer=8)),\n",
    "    (\"Medium\", GPTConfig(vocab_size=6400, n_embd=768, n_head=12, n_layer=12)),\n",
    "    (\"Large\", GPTConfig(vocab_size=6400, n_embd=1024, n_head=16, n_layer=24)),\n",
    "]\n",
    "\n",
    "print(\"不同规模的 GPT 参数量:\")\n",
    "print(\"-\" * 50)\n",
    "for name, config in configs:\n",
    "    # 估算公式\n",
    "    estimated = 12 * config.n_layer * config.n_embd ** 2\n",
    "    print(f\"{name}: n_embd={config.n_embd}, n_layer={config.n_layer}\")\n",
    "    print(f\"  估算: ~{estimated/1e6:.1f}M 参数\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 8. 运行完整测试"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python model_exercise.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 9. 验证清单\n",
    "\n",
    "完成本步骤后，你应该能够：\n",
    "\n",
    "- [ ] 画出 GPT 的架构图\n",
    "- [ ] 解释 Self-Attention 的计算过程（Q, K, V）\n",
    "- [ ] 解释为什么需要因果掩码\n",
    "- [ ] 实现 MLP、TransformerBlock、GPT\n",
    "- [ ] 计算给定配置的模型参数量\n",
    "\n",
    "---\n",
    "\n",
    "## 下一步\n",
    "\n",
    "理解模型架构后，进入 [Step 3: Pretrain](../step3_pretrain/) 学习如何预训练这个模型。"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
