{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 5: RLHF - 人类偏好对齐\n",
    "\n",
    "## 学习目标\n",
    "\n",
    "1. 理解人类偏好对齐的目标\n",
    "2. 理解 DPO 原理\n",
    "3. **动手实现** DPO Loss\n",
    "\n",
    "## 核心问题\n",
    "\n",
    "SFT 后模型能对话，但回答质量参差不齐。如何让模型输出更符合人类偏好？"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 1. 什么是人类偏好？\n",
    "\n",
    "给定同一个问题，人类会偏好某些回答：\n",
    "\n",
    "```\n",
    "问题: \"什么是AI？\"\n",
    "\n",
    "回答 A (chosen): \"AI是人工智能的缩写，是计算机科学的一个分支...\"\n",
    "回答 B (rejected): \"AI就是机器人。\"\n",
    "\n",
    "人类偏好: A > B\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 2. DPO vs PPO\n",
    "\n",
    "### 传统 RLHF (PPO)\n",
    "\n",
    "```\n",
    "偏好数据 → 训练 Reward Model → PPO 训练 → 对齐模型\n",
    "```\n",
    "\n",
    "问题：需要额外的 Reward Model，PPO 训练不稳定\n",
    "\n",
    "### DPO (Direct Preference Optimization)\n",
    "\n",
    "```\n",
    "偏好数据 → 直接优化 → 对齐模型\n",
    "```\n",
    "\n",
    "优点：无需 Reward Model，更简单稳定"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 3. DPO 核心公式\n",
    "\n",
    "$$L_{DPO} = -\\log \\sigma\\left(\\beta \\cdot \\left(\\log \\frac{\\pi(y_w|x)}{\\pi_{ref}(y_w|x)} - \\log \\frac{\\pi(y_l|x)}{\\pi_{ref}(y_l|x)}\\right)\\right)$$\n",
    "\n",
    "其中：\n",
    "- $y_w$: chosen（好回答）\n",
    "- $y_l$: rejected（差回答）\n",
    "- $\\pi$: 策略模型（训练中）\n",
    "- $\\pi_{ref}$: 参考模型（冻结）\n",
    "- $\\beta$: 温度系数\n",
    "\n",
    "### 直觉理解\n",
    "\n",
    "1. 增加 chosen 的概率\n",
    "2. 降低 rejected 的概率\n",
    "3. 不要偏离参考模型太远"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# DPO Loss 演示\n",
    "def demo_dpo_loss(policy_chosen, policy_rejected, ref_chosen, ref_rejected, beta=0.1):\n",
    "    \"\"\"DPO Loss 的计算过程\"\"\"\n",
    "    \n",
    "    # Step 1: 策略模型的 log ratio\n",
    "    pi_logratios = policy_chosen - policy_rejected\n",
    "    print(f\"策略模型 log ratio: {pi_logratios.item():.4f}\")\n",
    "    print(f\"  → 正值表示策略模型更偏好 chosen\")\n",
    "    \n",
    "    # Step 2: 参考模型的 log ratio  \n",
    "    ref_logratios = ref_chosen - ref_rejected\n",
    "    print(f\"\\n参考模型 log ratio: {ref_logratios.item():.4f}\")\n",
    "    \n",
    "    # Step 3: 相对改进\n",
    "    logits = pi_logratios - ref_logratios\n",
    "    print(f\"\\n相对改进: {logits.item():.4f}\")\n",
    "    print(f\"  → 正值表示策略模型比参考模型更偏好 chosen\")\n",
    "    \n",
    "    # Step 4: Loss\n",
    "    loss = -F.logsigmoid(beta * logits)\n",
    "    print(f\"\\nDPO Loss: {loss.item():.4f}\")\n",
    "    \n",
    "    return loss\n",
    "\n",
    "# 示例\n",
    "print(\"=\" * 50)\n",
    "print(\"DPO Loss 计算过程\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "demo_dpo_loss(\n",
    "    policy_chosen=torch.tensor(-1.0),    # 策略模型: chosen 概率较高\n",
    "    policy_rejected=torch.tensor(-2.0),  # 策略模型: rejected 概率较低\n",
    "    ref_chosen=torch.tensor(-1.5),       # 参考模型: 两者差异不大\n",
    "    ref_rejected=torch.tensor(-1.6),\n",
    "    beta=0.1\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1 练习：实现 DPO Loss\n",
    "\n",
    "去 `train_dpo_exercise.py`，完成 **TODO 1**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 测试你的实现\n",
    "import importlib\n",
    "import train_dpo_exercise\n",
    "importlib.reload(train_dpo_exercise)\n",
    "\n",
    "train_dpo_exercise.test_dpo_loss()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 4. 验证清单\n",
    "\n",
    "- [ ] 解释 RLHF 的目标（让模型输出符合人类偏好）\n",
    "- [ ] 解释 DPO 相比 PPO 的优势（无需 Reward Model）\n",
    "- [ ] 实现 DPO Loss\n",
    "- [ ] 理解 β 参数的作用（控制偏离程度）\n",
    "\n",
    "---\n",
    "\n",
    "## 下一步\n",
    "\n",
    "进入 [Step 6: VLM](../step6_vlm/)，学习如何扩展到多模态。"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
