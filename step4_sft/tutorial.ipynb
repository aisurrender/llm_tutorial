{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 4: SFT - 指令微调\n",
    "\n",
    "## 学习目标\n",
    "\n",
    "1. 理解 SFT 和预训练的区别\n",
    "2. **动手实现**只计算 Assistant 部分的 Loss\n",
    "3. 理解 LoRA 高效微调的原理\n",
    "\n",
    "## 核心问题\n",
    "\n",
    "预训练模型会\"续写\"，但不会\"对话\"。如何让它学会遵循指令？\n",
    "\n",
    "```\n",
    "预训练: \"今天天气\" → \"真好，适合出游...\"（续写）\n",
    "SFT 后: \"今天天气怎么样？\" → \"今天天气晴朗。\"（回答）\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 1. SFT 数据格式\n",
    "\n",
    "SFT 使用对话格式的数据：\n",
    "\n",
    "```json\n",
    "{\n",
    "  \"conversations\": [\n",
    "    {\"role\": \"system\", \"content\": \"你是一个助手。\"},\n",
    "    {\"role\": \"user\", \"content\": \"什么是AI？\"},\n",
    "    {\"role\": \"assistant\", \"content\": \"AI是人工智能...\"}\n",
    "  ]\n",
    "}\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 2. 关键：只计算 Assistant 部分的 Loss\n",
    "\n",
    "### 为什么？\n",
    "\n",
    "- 我们希望模型学会**如何回答**，而不是学会**如何提问**\n",
    "- User 的问题是输入上下文，不应该学习\n",
    "- 只有 Assistant 的回答需要优化\n",
    "\n",
    "### 实现方式\n",
    "\n",
    "```python\n",
    "# 输入: [system_tokens, user_tokens, assistant_tokens]\n",
    "# 标签: [-100, -100, ..., -100, assistant_tokens]\n",
    "#       ↑ 不计算 loss    ↑ 计算 loss\n",
    "\n",
    "# PyTorch 的 CrossEntropyLoss 会忽略 -100\n",
    "loss = F.cross_entropy(logits, labels, ignore_index=-100)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# 演示 ignore_index 的作用\n",
    "logits = torch.randn(5, 10)  # 5 个位置，10 个类别\n",
    "labels = torch.tensor([2, -100, -100, 5, 3])  # 只有位置 0, 3, 4 计算 loss\n",
    "\n",
    "loss = F.cross_entropy(logits, labels, ignore_index=-100)\n",
    "print(f\"Loss: {loss.item():.4f}\")\n",
    "print(f\"只计算了 3 个位置的 loss（-100 被忽略）\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 练习：实现 _format_conversation\n",
    "\n",
    "去 `data_exercise.py`，完成 **TODO 1**：\n",
    "\n",
    "构建 labels，使得只有 Assistant 部分计算 loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 测试你的实现\n",
    "import importlib\n",
    "import data_exercise\n",
    "importlib.reload(data_exercise)\n",
    "\n",
    "data_exercise.test_sft_dataset()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 3. LoRA：高效微调\n",
    "\n",
    "### 问题\n",
    "\n",
    "全参数微调需要：\n",
    "- 大量显存（存储梯度和优化器状态）\n",
    "- 每个任务都要保存完整模型\n",
    "\n",
    "### LoRA 的解决方案\n",
    "\n",
    "不训练原始权重，而是训练一个低秩\"旁路\"：\n",
    "\n",
    "```\n",
    "原始:  y = Wx           (W 是冻结的)\n",
    "LoRA:  y = Wx + BAx     (只训练 B 和 A)\n",
    "\n",
    "W: [n, m]  → n×m 个参数（冻结）\n",
    "B: [n, r]  → n×r 个参数（训练）\n",
    "A: [r, m]  → r×m 个参数（训练）\n",
    "\n",
    "当 r << n, m 时，参数量大大减少\n",
    "例如: n=m=4096, r=8 → 参数减少 99.6%\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "# LoRA 原理演示\n",
    "n, m, r = 4096, 4096, 8\n",
    "\n",
    "# 原始参数量\n",
    "original_params = n * m\n",
    "print(f\"原始参数量: {original_params:,} ({original_params/1e6:.1f}M)\")\n",
    "\n",
    "# LoRA 参数量\n",
    "lora_params = n * r + r * m\n",
    "print(f\"LoRA 参数量: {lora_params:,} ({lora_params/1e3:.1f}K)\")\n",
    "\n",
    "print(f\"\\n参数减少: {100 * (1 - lora_params / original_params):.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 4. 验证清单\n",
    "\n",
    "完成本步骤后，你应该能够：\n",
    "\n",
    "- [ ] 解释 SFT 和预训练的区别\n",
    "- [ ] 解释为什么只计算 Assistant 部分的 Loss\n",
    "- [ ] 实现 labels 的构建逻辑\n",
    "- [ ] 解释 LoRA 的原理和优势\n",
    "\n",
    "---\n",
    "\n",
    "## 下一步\n",
    "\n",
    "进入 [Step 5: RLHF](../step5_rlhf/)，学习如何让模型输出更符合人类偏好。"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
