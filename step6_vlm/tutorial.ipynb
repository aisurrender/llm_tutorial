{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 6: VLM - 多模态扩展\n",
    "\n",
    "## 学习目标\n",
    "\n",
    "1. 理解 VLM 的整体架构\n",
    "2. 理解 Vision Encoder、Projection、LLM 的作用\n",
    "3. **动手实现** Projection Layer\n",
    "\n",
    "## 核心问题\n",
    "\n",
    "如何让语言模型\"看懂\"图片？\n",
    "\n",
    "```\n",
    "图片 → Vision Encoder → 图像特征 → Projection → \"图像 Token\" → LLM → 文本输出\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 1. VLM 架构\n",
    "\n",
    "```\n",
    "┌─────────────────────────────────────────────────┐\n",
    "│   图片 (224×224×3)                               │\n",
    "│         ↓                                       │\n",
    "│   ┌─────────────────┐                           │\n",
    "│   │ Vision Encoder  │  图片 → [196, 768] 特征   │\n",
    "│   │   (CLIP-ViT)    │  196 = 14×14 个 patch     │\n",
    "│   └─────────────────┘                           │\n",
    "│         ↓                                       │\n",
    "│   ┌─────────────────┐                           │\n",
    "│   │   Projection    │  768 → text_dim           │\n",
    "│   │   (可训练)      │  映射到文本空间            │\n",
    "│   └─────────────────┘                           │\n",
    "│         ↓                                       │\n",
    "│   [img_1, ..., img_196, text_1, text_2, ...]   │\n",
    "│         ↓                                       │\n",
    "│   ┌─────────────────┐                           │\n",
    "│   │      LLM        │  生成文本回复              │\n",
    "│   └─────────────────┘                           │\n",
    "│         ↓                                       │\n",
    "│   \"这是一只猫...\"                                │\n",
    "└─────────────────────────────────────────────────┘\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 2. 各组件的作用\n",
    "\n",
    "### Vision Encoder\n",
    "\n",
    "- **作用**：将图片编码为特征向量序列\n",
    "- **类比**：\"翻译官\"，把图片这门\"外语\"翻译成特征\n",
    "- **常用**：CLIP-ViT、SigLIP\n",
    "- **输出**：[num_patches, vision_dim]，如 [196, 768]\n",
    "\n",
    "### Projection\n",
    "\n",
    "- **作用**：将视觉特征映射到文本嵌入空间\n",
    "- **为什么需要**：Vision Encoder 和 LLM 的维度不同\n",
    "- **实现**：线性层或 MLP\n",
    "\n",
    "### LLM\n",
    "\n",
    "- **作用**：接收图像+文本 token，生成回复\n",
    "- **输入**：[image_tokens, text_tokens]\n",
    "- **把图像 token 当作\"特殊的文本 token\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "# 演示：维度变换\n",
    "vision_dim = 768  # CLIP-ViT 输出\n",
    "text_dim = 512    # LLM 嵌入维度\n",
    "num_patches = 196 # 14 × 14\n",
    "\n",
    "# 模拟图像特征\n",
    "image_features = torch.randn(1, num_patches, vision_dim)\n",
    "print(f\"Vision Encoder 输出: {image_features.shape}\")\n",
    "\n",
    "# Projection\n",
    "projection = nn.Linear(vision_dim, text_dim)\n",
    "image_embeds = projection(image_features)\n",
    "print(f\"Projection 后: {image_embeds.shape}\")\n",
    "\n",
    "# 模拟文本嵌入\n",
    "text_len = 20\n",
    "text_embeds = torch.randn(1, text_len, text_dim)\n",
    "print(f\"文本嵌入: {text_embeds.shape}\")\n",
    "\n",
    "# 拼接\n",
    "combined = torch.cat([image_embeds, text_embeds], dim=1)\n",
    "print(f\"拼接后（LLM 输入）: {combined.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 3. 为什么冻结 Vision Encoder？\n",
    "\n",
    "1. **利用预训练知识**：CLIP 已经学会了强大的视觉表示\n",
    "2. **减少训练成本**：只训练 Projection 和 LLM\n",
    "3. **防止遗忘**：避免破坏预训练的视觉能力"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 4. 练习：实现 Projection Layer\n",
    "\n",
    "去 `model_exercise.py`，完成 **TODO 1**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 测试你的实现\n",
    "import importlib\n",
    "import model_exercise\n",
    "importlib.reload(model_exercise)\n",
    "\n",
    "model_exercise.test_projection()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 5. 验证清单\n",
    "\n",
    "- [ ] 画出 VLM 的架构图\n",
    "- [ ] 解释 Vision Encoder、Projection、LLM 各自的作用\n",
    "- [ ] 解释为什么通常冻结 Vision Encoder\n",
    "- [ ] 实现 Projection Layer\n",
    "\n",
    "---\n",
    "\n",
    "## 恭喜完成！\n",
    "\n",
    "你已经完成了 LLM/VLM 训练全流程的学习：\n",
    "\n",
    "```\n",
    "Step 1: Tokenizer    ✓ 文本 → Token\n",
    "Step 2: GPT Model    ✓ Transformer 架构\n",
    "Step 3: Pretrain     ✓ 下一个词预测\n",
    "Step 4: SFT          ✓ 指令微调\n",
    "Step 5: RLHF         ✓ 人类偏好对齐\n",
    "Step 6: VLM          ✓ 多模态扩展\n",
    "```\n",
    "\n",
    "现在你已经掌握了大模型训练的核心知识！"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
